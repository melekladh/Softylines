{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0242bf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5106261",
   "metadata": {},
   "source": [
    "compenents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f56429e",
   "metadata": {},
   "source": [
    "chatmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ba012c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from C:/Users/Taki Academy/taki/mistral-7b-v0.1.Q8_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q8_0\n",
      "print_info: file size   = 7.17 GiB (8.50 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: printing all EOG tokens:\n",
      "load:   - 2 ('</s>')\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-v0.1\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q8_0) (and 290 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  7338.64 MiB\n",
      "...................................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 2048\n",
      "llama_context: n_ctx_per_seq = 2048\n",
      "llama_context: n_batch       = 64\n",
      "llama_context: n_ubatch      = 8\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 2048 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   256.00 MiB\n",
      "llama_kv_cache_unified: size =  256.00 MiB (  2048 cells,  32 layers,  1/1 seqs), K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 2328\n",
      "llama_context: worst-case: n_tokens = 8, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    8, n_seqs =  1, n_outputs =    8\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    8, n_seqs =  1, n_outputs =    8\n",
      "llama_context:        CPU compute buffer size =     3.06 MiB\n",
      "llama_context: graph nodes  = 1126\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '7', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"C:/Users/Taki Academy/taki/mistral-7b-v0.1.Q8_0.gguf\",\n",
    "    n_ctx=2048,\n",
    "    temperature=0.75,\n",
    "    max_tokens=2000,\n",
    "    top_p=1,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    "  \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973b3ca2",
   "metadata": {},
   "source": [
    "emebdding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22eb7c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d5cc366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\taki academy\\taki\\.venv\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\taki academy\\taki\\.venv\\lib\\site-packages (from sentence-transformers) (4.54.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\taki academy\\taki\\.venv\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\taki academy\\taki\\.venv\\lib\\site-packages (from sentence-transformers) (2.7.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\taki academy\\taki\\.venv\\lib\\site-packages (from sentence-transformers) (1.7.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\taki academy\\taki\\.venv\\lib\\site-packages (from sentence-transformers) (1.16.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\taki academy\\taki\\.venv\\lib\\site-packages (from sentence-transformers) (0.34.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\taki academy\\taki\\.venv\\lib\\site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\taki academy\\taki\\.venv\\lib\\site-packages (from sentence-transformers) (4.14.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\taki academy\\taki\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\taki academy\\taki\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\taki academy\\taki\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\taki academy\\taki\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\taki academy\\taki\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in c:\\users\\taki academy\\taki\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\taki academy\\taki\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\taki academy\\taki\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\taki academy\\taki\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\taki academy\\taki\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\taki academy\\taki\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\taki academy\\taki\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\taki academy\\taki\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\taki academy\\taki\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\taki academy\\taki\\.venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\taki academy\\taki\\.venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\taki academy\\taki\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\taki academy\\taki\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\taki academy\\taki\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\taki academy\\taki\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.8.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\taki academy\\taki\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\taki academy\\taki\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3627a8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Taki Academy\\taki\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77649570",
   "metadata": {},
   "source": [
    "vector store "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66e34bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9964f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43e070d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#indexing \n",
    "#loading document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62b02d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-community jq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "269be123",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path=\"C:/Users/Taki Academy/taki/myjson.json\",\n",
    "    jq_schema=\".summary\",\n",
    "    text_content=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9a2463",
   "metadata": {},
   "source": [
    "load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8a95375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='كان عدد المشتركين في شبكة الاتصالات في الولايات المتحدة الأمريكية 646 مشترك لكل 1,000 ساكن، بينما كان عدد المشتركين في الهاتف القار 488 مشترك لكل 1,000 ساكن، بينما كان عدد المشتركين في التلفزة بالكابل 488 مشترك لكل 1,000 ساكن، بينما كان عدد المشتركين في التلفزة بالكابل 488 مشترك لكل 1,000 ساكن، بينما كان عدد المشتركين في التلفزة بالكابل 488 مشترك لكل 1,000 ساكن، بينما كان عدد المشتركين في التلفزة بالكابل 488 مشترك لكل 1,000 ساكن، بينما كان عدد المشتركين في التلفزة بالكابل 488 مشترك لكل 1,000 ساكن، بينما كان عدد المشتركين في التلفزة بالكابل البصرية والأقمار صناعية في الولايات متحدة الأمريكية في الولايات متحدة الأمريكية في الولايات متحدة الأمريكية في الولايات متحدة الأمريكية في الولايات متحدة الأمريكية في الولايات متحدة الأمريكية في' metadata={'source': 'C:\\\\Users\\\\Taki Academy\\\\taki\\\\myjson.json', 'seq_num': 1}\n"
     ]
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "print(docs[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d85315",
   "metadata": {},
   "source": [
    "splitting documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0a3d5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 1 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece2637c",
   "metadata": {},
   "source": [
    "Storing documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ade8984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4bee46a6-42ae-41fa-8d11-b52ff7193a3f\n"
     ]
    }
   ],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(document_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927f22e1",
   "metadata": {},
   "source": [
    "2. Retrieval and Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca71e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain import hub\n",
    "\n",
    "# # N.B. for non-US LangSmith endpoints, you may need to specify\n",
    "# # api_url=\"https://api.smith.langchain.com\" in hub.pull.\n",
    "# from langsmith import Client\n",
    "# client = Client(api_key=\"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\")\n",
    "# prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# example_messages = prompt.invoke(\n",
    "#     {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}\n",
    "# ).to_messages()\n",
    "\n",
    "# assert len(example_messages) == 1\n",
    "# print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c6230c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll use LangGraph to tie together the retrieval and generation steps into a single application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2c82fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The state of our application controls what data is input to the application, transferred between steps, and output by the application.\n",
    "# from langchain_core.documents import Document\n",
    "# from typing_extensions import List, TypedDict\n",
    "\n",
    "\n",
    "# class State(TypedDict):\n",
    "#     question: str\n",
    "#     context: List[Document]\n",
    "#     answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd88d158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"عدد المشتركين في شبكة الاتصالات في الولايات المتحدة الأمريكية\"\n",
    "\n",
    "# llm.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6cc7c5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Taki Academy\\taki\\.venv\\Lib\\site-packages\\langsmith\\client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "Llama.generate: 773 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " عدد المشتركين في شبكة الاتصالات في الولايات المتحدة الأمريكية 125\n",
      "Human Rating: 1\n",
      "Prediction: عدد المشتركين في شبكة الاتصالات في الولايات المتحدة الأمريكية 125\n",
      "Human Rating: 1\n",
      "Prediction: عدد المشتركين في شبكة الاتصالات في الولايات المتحدة الأمريكية 125\n",
      "Human Rating: 1\n",
      "Prediction: عدد المشتركين في شبكة الاتصالات في الولايات المتحدة الأمريكية 125\n",
      "Human Rating: 1\n",
      "Prediction: عدد المشتركين في شبكة الاتصالات في الولايات المتحدة الأمريكية 125\n",
      "Human Rating: 1\n",
      "Prediction: عدد المشتركين في شبكة الاتصالات في الولايات المتحدة الأمريكية 125\n",
      "Human Rating: 1\n",
      "Prediction: عدد المشتركين في شبكة الاتصالات في الولايات المتحدة الأمريكية 125\n",
      "Human Rating: 1\n",
      "Prediction: عدد المشتركين في شبكة الاتصالات في الولايات المتحدة الأمريكية 125\n",
      "Human Rating: 1\n",
      "Prediction: عدد المشتركين في شبكة الاتصالات في الولايات المتحدة الأمريكية 125\n",
      "Human Rating: 1\n",
      "Prediction: عدد المشتركين في شبكة الاتصالات في الولايات المتحدة الأمريكية 125\n",
      "Human Rating: 1\n",
      "Prediction: عدد المشتركين في شبكة الاتصالات في الولايات المتحدة الأمريكية 125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   53043.25 ms\n",
      "llama_perf_context_print: prompt eval time =  107188.40 ms /     2 tokens (53594.20 ms per token,     0.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =  178549.00 ms /   715 runs   (  249.72 ms per token,     4.00 tokens per second)\n",
      "llama_perf_context_print:       total time =  180172.16 ms /   717 tokens\n",
      "llama_perf_context_print:    graphs reused =        693\n"
     ]
    }
   ],
   "source": [
    "question = \"عدد المشتركين في شبكة الاتصالات في الولايات المتحدة الأمريكية\"\n",
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "retrieved_docs = vector_store.similarity_search(question,k=3)\n",
    "docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "prompt = prompt.invoke({\"question\": question, \"context\": docs_content})\n",
    "answer = llm.invoke(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
